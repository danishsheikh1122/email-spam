# -*- coding: utf-8 -*-
"""emailspamdetextion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dnF5SSRYp7nx9N3EzR-kijp2WgwSFwTS
"""

import nltk

# Download the required 'punkt_tab' data
nltk.download('punkt_tab')

import numpy as np
import pandas as pd
import re
import string
import nltk
import tensorflow as tf
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
#from keras.preprocessing.text import Tokenizer   # This line caused the error
#from keras.preprocessing.sequence import pad_sequences # This line caused the error
from tensorflow.keras.preprocessing.text import Tokenizer # Import from tensorflow.keras.preprocessing instead
from tensorflow.keras.preprocessing.sequence import pad_sequences # Import from tensorflow.keras.preprocessing instead
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, SpatialDropout1D
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Download stopwords
nltk.download('stopwords')
nltk.download('punkt')

# Load dataset
df = pd.read_csv("/content/spam.csv", encoding="latin-1")
df = df.iloc[:, :2]
df.columns = ['target', 'message']

# Encode labels (spam = 1, ham = 0)
label_encoder = LabelEncoder()
df['target_num'] = label_encoder.fit_transform(df['target'])

def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\.\S+', '', text)  # Remove URLs
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    return text

stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    words = word_tokenize(text)
    return ' '.join(word for word in words if word not in stop_words)

# Apply preprocessing
df['clean_text'] = df['message'].apply(clean_text).apply(remove_stopwords)

# Tokenization & Padding
max_words = 5000  # Vocabulary size
max_len = 150  # Max length of sequences

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(df['clean_text'])
X = tokenizer.texts_to_sequences(df['clean_text'])
X = pad_sequences(X, maxlen=max_len)

y = df['target_num'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build LSTM Model
model = Sequential([
    Embedding(max_words, 50, input_length=max_len),
    SpatialDropout1D(0.2),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_test, y_test))

# Evaluate
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

def predict_spam(text):
    text = clean_text(text)
    text = remove_stopwords(text)
    sequence = tokenizer.texts_to_sequences([text])
    sequence = pad_sequences(sequence, maxlen=max_len)
    prediction = model.predict(sequence)[0][0]

    return "Spam" if prediction > 0.5 else "Not Spam"

# Example Usage
sample_text = '''You can host your NLP model using FastAPI and deploy it on a cloud platform like Render, Hugging Face Spaces, AWS, or Google Cloud. Hereâ€™s a step-by-step guide:

1. Save Your NLP Model
If you're using scikit-learn, spaCy, or a transformer model (like GPT or BERT), save your trained model:
scam scam scam scam
For scikit-learn or spaCy:
python
Copy
Edit
'''
print(predict_spam(sample_text))

!pip install FastAPI

"""imp code starts here

"""

import numpy as np
import pandas as pd
import re
import string
import nltk
import tensorflow as tf
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, SpatialDropout1D
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pickle

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('punkt')

# Load dataset
df = pd.read_csv("/content/spam.csv", encoding="latin-1")
df = df.iloc[:, :2]
df.columns = ['target', 'message']

# Encode labels (spam = 1, ham = 0)
label_encoder = LabelEncoder()
df['target_num'] = label_encoder.fit_transform(df['target'])

# Define cleaning functions
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\.\S+', '', text)  # Remove URLs
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    return text

stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    words = word_tokenize(text)
    return ' '.join(word for word in words if word not in stop_words)

# Apply text preprocessing
df['clean_text'] = df['message'].apply(clean_text).apply(remove_stopwords)

# Tokenization & Padding
max_words = 5000  # Vocabulary size
max_len = 150     # Max length of sequences

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(df['clean_text'])
X = tokenizer.texts_to_sequences(df['clean_text'])
X = pad_sequences(X, maxlen=max_len)

y = df['target_num'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build LSTM Model
model = Sequential([
    Embedding(max_words, 50, input_length=max_len),
    SpatialDropout1D(0.2),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_test, y_test))

# Evaluate
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

# Save model and tokenizer
model.save("spam_model.h5")  # Save model
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

print("Model and Tokenizer saved successfully.")

!pip install fastapi nest_asyncio pyngrok uvicorn tensorflow
!pip install nltk

!ngrok authtoken 1qNrwVnN4jWkwHVCbl3PblvohQH_3rK9SNEthBfXLYp1FAYhn

from fastapi import FastAPI
from pydantic import BaseModel
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
import pickle
import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import os
import nltk

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('punkt')

# Initialize FastAPI app
app = FastAPI()

# Define request body model
class TextInput(BaseModel):
    text: str

# Verify model and tokenizer files exist
if not os.path.exists("spam_model.h5") or not os.path.exists("tokenizer.pickle"):
    raise FileNotFoundError("Model or tokenizer file not found.")

# Load your pre-trained model and tokenizer
model = tf.keras.models.load_model("spam_model.h5")
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

max_len = 150  # Set this to your model's expected input length

# Function to clean and preprocess text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\.\S+', '', text)  # Remove URLs
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    return text

stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    words = word_tokenize(text)
    return ' '.join(word for word in words if word not in stop_words)

# Define the prediction function
def predict_spam(text):
    text = clean_text(text)
    text = remove_stopwords(text)
    sequence = tokenizer.texts_to_sequences([text])
    sequence = pad_sequences(sequence, maxlen=max_len)
    prediction = model.predict(sequence)[0][0]
    return "Spam" if prediction > 0.5 else "Not Spam"

# Define API endpoint
@app.post("/predict")
async def get_prediction(input: TextInput):
    result = predict_spam(input.text)
    return {"result": result}

model.save('spam_model.keras')  # Save your model
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle)

!ls -lh spam_model.keras tokenizer.pickle

model.save("spam_model.keras")
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

!pip install fastapi uvicorn nest_asyncio pyngrok

# Save the trained model
model.save("spam_model.h5")

# Save the tokenizer
import pickle
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

from fastapi import FastAPI
from pydantic import BaseModel
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import numpy as np

nltk.download('stopwords')
nltk.download('punkt')

app = FastAPI()

# Load the saved model and tokenizer
model = load_model("spam_model.h5")

with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

max_len = 150

# Define input schema
class TextInput(BaseModel):
    text: str

# Text Cleaning
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)
    text = re.sub(r'\d+', '', text)
    return text

stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    words = word_tokenize(text)
    return ' '.join(word for word in words if word not in stop_words)

# Prediction Function
def predict_spam(text):
    text = clean_text(text)
    text = remove_stopwords(text)
    sequence = tokenizer.texts_to_sequences([text])
    sequence = pad_sequences(sequence, maxlen=max_len)
    prediction = model.predict(sequence)[0][0]
    return "Spam" if prediction > 0.5 else "Not Spam"

# Define FastAPI Route
@app.post("/predict")
async def get_prediction(input: TextInput):
    result = predict_spam(input.text)
    return {"result": result}

import nest_asyncio
from pyngrok import ngrok
import uvicorn

# Apply nest_asyncio to allow running in Colab
nest_asyncio.apply()

# Get your ngrok token from https://dashboard.ngrok.com/get-started/setup
NGROK_AUTH_TOKEN = "1qNrwVnN4jWkwHVCbl3PblvohQH_3rK9SNEthBfXLYp1FAYhn"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Open an ngrok tunnel to the FastAPI server
public_url = ngrok.connect(8000).public_url
print(f"Public URL: {public_url}")

# Run Uvicorn server
uvicorn.run(app, host='0.0.0.0', port=8000)

"""just run above code"""

from google.colab import drive
drive.mount('/content/drive')